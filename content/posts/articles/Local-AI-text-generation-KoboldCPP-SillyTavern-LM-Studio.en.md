---
draft: false
# url: "" # slug
title: "Local AI text generation. KoboldCPP & SillyTavern. LM Studio"
description: "How to run local AI text generation installation on your PC."
summary: "How to run local AI text generation installation on your PC."
date: 2025-03-30
# lastmod: 2001-01-29
categories: ["Long Read", "AI"] # ["cat 1", "cat 2"]
tags: ["AI Text Gen"] # ['tag 1', 'tag 2']
author: ["nozsh"] # ['Me', 'You'] multiple authors
# authorURL: [""] # ['link author 1', 'link author 2'], ex. ['', 'https://example.com']
# canonicalURL: "yourself"
# CanonicalLinkText: "Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº:"
# weight: 1
# robotsNoIndex: true

# sha1: "" # for giscus

showToc: true
TocOpen: false
# hidemeta: true
# comments: false
# disableHLJS: true
# disableShare: true
# hideSummary: true
# hideFooter: true
# searchHidden: true
# ShowCodeCopyButtons: false
# ShowReadingTime: false
# ShowWordCount: false
# hideAuthor: true
# ShowBreadCrumbs: false
ShowPostNavLinks: false
# ShowRssButtonInSectionTermList: false
# ShowCanonicalLink: true
# UseHugoToc: false
# byai: true
cover:
  image: "@img/local-ai-text-generation-koboldcpp-sillytavern-lm-studio-cover.webp" # image path/url
  width: "2048" # only for img from url; EX: 1920
  height: "1296" # only for img from url; EX: 1080
  alt: "Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ AI Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°. KoboldCPP & SillyTavern. LM Studio - Cover" # alt text
  caption: "Cover generated by AI" # display caption under cover
  relative: true # when using page bundles set this to true
  hidden: false # only hide on current single page
---

{{< ahtung/badEn >}}

{{< callout/custom "ğŸ”¥" "#000" "#DAE2F8" "linear-gradient(62deg, #DAE2F8 0%, #D6A4A4 100%);" >}}
In this article, you'll learn how to set up a local text generation system on your computer -- **with minimal technical knowledge required**.
{{< /callout/custom >}}

## Introduction: About KoboldCPP & SillyTavern, LM Studio

**[KoboldCPP](https://github.com/LostRuins/koboldcpp?sl)** is a user-friendly tool for running text generation models in GGML and GGUF formats, built on top of [llama.cpp](https://github.com/ggml-org/llama.cpp?sl).

**llama.cpp** is written **pure in C/C++** and has **no external dependencies**, which allows it to run with high performance.

KoboldCPP is especially useful for running local LLMs on systems with limited VRAM, as it can utilize the CPU instead of relying solely on the GPU.

While KoboldCPP may be slower than other backends that load models exclusively into VRAM, it makes it possible to run large LLMs on less powerful machines.

KoboldCPP includes its own interface (UI), but it's not as flexible, feature-rich, or (in the author's opinion) as comfortable to use as SillyTavern.

**[SillyTavern](https://github.com/SillyTavern/SillyTavern?sl)** is a UI designed for interacting with text generation models via API (KoboldAI/CPP, OpenAI, OpenRouter, and others).

It integrates with in-chat image generation tools like [WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui?sl) from [A1111](https://github.com/AUTOMATIC1111?sl) and [ComfyUI](https://github.com/comfyanonymous/ComfyUI?sl), and also supports TTS. It can work either as a standalone system (non-AI) or generate voice using AI through APIs.

SillyTavern features a highly customizable interface, includes powerful tools like lorebooks (lorebook and world info), supports automatic message translation via Google, DeepL, and other services, and offers a wide range of community-created extensions and add-ons.

KoboldCPP + SillyTavern is a versatile, flexible setup that's suitable for just about anything -- including immersive RP.

**[LM Studio](https://lmstudio.ai/?sl)** is an incredibly simple and user-friendly tool -- install it, launch it, and you're good to go.

It comes with built-in support for models with "reasoning" capabilities, no setup required.

LM Studio is great for testing models and handling both technical and general tasks like coding, answering questions, writing, rewriting, and more. 

As a backend, LM Studio is less flexible than KoboldCPP. And as a frontend, it's not as customizable as SillyTavern -- but it's minimalist and pleasant to use.

{{< callout/hint >}}
This article covers the setup process for both Windows and Linux (Debian/Arch).
{{< /callout/hint >}}


## Installing Git (Bash), NodeJS and Python

### NodeJS

{{< callout/note >}}
NodeJS is required to run SillyTavern. If you don't plan to use SillyTavern, you won't need NodeJS.
{{< /callout/note >}}

#### Windows

For Windows, [download the installer](https://nodejs.org/en/download?sl) for your system architecture.

I recommend using the LTS version.

Run the installer, click "Next" through the steps, **uncheck** the box at the end that says "Automatically install the necessary tools...", and complete the installation.

#### Linux

On Linux, you can install NodeJS using Volta or [any other](https://nodejs.org/en/download?sl) method you prefer.

{{< callout/note >}}
The version of Node available in package managers (like apt, pacman, etc.) may be outdated.
{{< /callout/note >}}

Installing Volta:

```bash
curl https://get.volta.sh | bash
```

Installing NodeJS and NPM:

```bash
volta install node@x
```

Where "x" is the major version of Node you want to install, which you can check on the [NodeJS homepage](https://nodejs.org/?sl).

For example, Node.js LTSÂ v<u>**22**</u>.14.0:

```bash
volta install node@22
```

### Git

{{< callout/note >}}
Git is needed to clone the SillyTavern repository and for easy updates, but you can also download the repository manually.
{{< /callout/note >}}

#### Windows

For Windows, you need to [download Git](https://git-scm.com/downloads/win?sl), either the "Standalone" installer or the "Portable" version. I recommend the portable version, but you'll have to update your environment variables manually -- more on that later.

#### Linux

If you're a Linux user, Git is probably already installed. You can check by running `git -v`. If it's not installed:

**Debian:**

```bash
apt update
apt install git
```

**Arch:**

```bash
pacman -Syu
pacman -S git
```

### Python

{{< callout/note >}}
To run the "unpacked" version of KoboldCPP, Python is required.

I recommend installing Python and using the "unpacked" KoboldCPP -- this will improve performance.

More details on this will come later.
{{< /callout/note >}}

{{< callout/note >}}
At the time of writing this article, I'm using [Python 3.11.9](https://www.python.org/downloads/release/python-3119/?sl).
{{< /callout/note >}}

{{< callout/hint >}}
If you're using the latest Python version and something isn't working, try downgrading the minor version -- x.<u>**x**</u>.x.
{{< /callout/hint >}}

#### Windows

For Windows, you need to download and install [Python](https://www.python.org/downloads/windows/?sl).

Download the Â«Windows InstallerÂ» for the Python version you want from the Â«Stable ReleasesÂ» column. *If you don't need a specific version, just get the latest one.*

During the installation, click Â«Customize installationÂ» and check the boxes for Â«pipÂ», Â«tcl/tkÂ», and Â«Add Python to environment variablesÂ»:

![Python installer window](@img/windows-python-installing.avif)

You can choose the installation location as you prefer.

#### Linux

Just like with Git, Python is usually already installed on Linux distros, but it might be an older version. You can check by running:

```bash
python -V
# OR
python2 -V
# OR
python3 -V
```

Python 2 is an old version. If you don't have Python installed, or the version is outdated, then:

**Debian:**

```bash
apt update
apt install python3
apt install python3-pip # Might need to
```

**Arch:**

```bash
pacman -Syu
pacman -S python
pacman -S python-pip # Might need to
```

### Setting up environment variables

After installing Git, NodeJS, and Python, you may need to add them to your environment variables. This lets you run them without specifying the full path to their binaries. Programs will also be able to use simple commands like `python`, `node`, and `git`.

Usually, the Python and NodeJS installers update the environment variables automatically -- unless you disabled that option. The same goes for Git if you installed it via the installer instead of using the portable version.

#### Windows

{{< callout/hint >}}
If you installed Python and NodeJS using their installers as described earlier, but downloaded Git as a portable version, you only need to add Git to your environment variables.
{{< /callout/hint >}}

1. <kbd>Win</kbd> + <kbd>R</kbd>
2. `systempropertiesadvanced`
3. Environment Variables
4. User variables
5. Path
6. New

- Git: `path\to\git\bin\` and `path\to\git\cmd\`
- NodeJS: `path\to\nodejs\`
- Python: `path\to\python\` and `path\to\python\Scripts\`

***One line -- one path!***

#### Linux

If you installed Git and Python through a package manager (apt, pacman, etc.) or they were already on your system, and NodeJS was installed via the official site (including through Volta), everything should be fine.

But if that's not the case:

Open `.bashrc` in the nano editor:

```bash
nano ~/.bashrc
```

At the end of the file add:

```bash
export PATH=$PATH:/path/to
```

Update `.bashrc`:

```bash
source ~/.bashrc
```

Usually the paths are:

- Git: `/usr/bin/git`
- NodeJS: `/usr/bin/node`
- Python: `/usr/bin/python` or `/usr/bin/python3`

But they might be different.

You can try running this command to find out where the program is located:

```bash
which <...>
```

Example of `.bashrc`:

```bash
...

export PATH=$PATH:/usr/bin/git
export PATH=$PATH:/usr/bin/node
export PATH=$PATH:/usr/bin/python3
```

#### Check

Type the following in the terminal:

```bash
$ git -v
git version 2.48.1.windows.1

$ node -v
v22.14.0

$ npm -v
10.9.2

$ python -V
Python 3.11.9
```

If you get the version number **instead of an error** like this:

```text
'x' is not recognized as an internal or external command,
operable program or batch file.
``` 

```bash
-bash: x: command not found
```

That means everything is working.


## Installing and Running KoboldCPP + SillyTavern & LM Studio

To start, I recommend thinking about your directories where to put what and set up something like this:

```bash
| - AI                <== Root
|
| - - Backend         <== Backend
| - - - LM Studio     <== Installation of LM Studio
| - - - KoboldCPP     <== KoboldCPP dir
| - - - - Loose       <== Unpacked KoboldCPP
| - - - - Settings    <== KoboldCPP configs (.kcpps)
| - - - - .exe        <== KoboldCPP binary file
|
| - - UI              <== Frontend
| - - - SillyTavern   <== SillyTavern dir
|
| - - Models          <== Models directory
| - - - Text          <== Text models
```

### KoboldCPP

Go to the [releases page](https://github.com/LostRuins/koboldcpp/releases?sl) on the GitHub [KoboldCPP repository](https://github.com/LostRuins/koboldcpp?sl).

Download the binary for your system, choosing based on the release notes.

Run the binary file:

{{< imgs/imgc
width=""
caption="You can see the logs here"
alt="KoboldCPP - Start Terminal"
src="@img/koboldcpp-start-terminal-logs-1.avif" >}}

{{< imgs/imgc
width=""
caption="KoboldCPP - GUI"
alt="KoboldCPP - Start GUI"
src="@img/koboldcpp-start-gui-2.avif" >}}

The startup isn't fast because these are packed Python files (code), the Python interpreter, and other files bundled into one.

You can also interact with the binary through the terminal, for example:

```batch
koboldcpp_cu12.exe --help
```

To unpack KoboldCPP, go to Â«ExtraÂ» => Â«Unpack KoboldCpp To FolderÂ» and choose an empty directory to extract to.

Now you can run KoboldCPP **much faster** using Python:

```bash
python koboldcpp.py --help
```

However, in this case the GUI won't work, GUI only works when running the binary file.

If you don't want to deal with the terminal and typing lots of flags, there's an option to configure everything through the GUI.

Start the binary file and set up everything via the GUI, then save the config (click the Â«SaveÂ» button).

After that, you can use this command:

```bash
python koboldcpp.py --config "/path/to/myconfig.kcpps"  
```

This will launch KoboldCPP using the previously created config.

If you need to edit the config, you can do it the same way through the GUI (by loading it with the Â«LoadÂ» button, making changes, and then saving), or simply open the config file in a text editor to swap models or change the context size.

### SillyTavern

Download the source code manually from the GitHub [repository](https://github.com/SillyTavern/SillyTavern?sl) or use git:

```bash
git clone https://github.com/SillyTavern/SillyTavern -b release
```

This command clones the repository into a folder named "SillyTavern" in the same directory where the command was run.

Running on Windows:

Double-click Â«Start.batÂ» or run in the terminal:

```batch
Start.bat
```

Running on Linux:

Make the file executable:

```bash
chmod +x start.sh
```

Run:

```bash
./start.sh
```

#### Base settings

Open the Â«API ConnectionsÂ» tab (at the top):

- API: Text Completion
- API Type: KoboldCpp
- API URL: KoboldCPP API URL

You can find the Â«API URLÂ» in the terminal after starting KoboldCPP, but usually it's `:5001` if that port was free.

I highly recommend enabling Â«Derive context size from backendÂ» so that the Â«Context (tokens)Â» value is automatically fetched from the backend -- meaning from the KoboldCPP setting where you set the context size.

Â«Auto-connect to Last ServerÂ» is also a useful option. It lets SillyTavern connect automatically to the last backend server on startup, so you don't have to tap Â«ConnectÂ» manually every time.

{{< imgs/imgc
width=""
caption="SillyTavern - tab with backend API settings"
alt="SillyTavern - tab with backend API settings"
src="@img/sillytavern-api.avif" >}}

Tap Â«ConnectÂ», and if you see ğŸŸ¢ and the model name, that means everything is working.

### LM Studio

Go to the [LM Studio website](https://lmstudio.ai/download?sl), select your OS and architecture, then download and install it.

{{< callout/note >}}
I recommend disabling Â«Enable Local LLM ServiceÂ» if you don't want the service to run when LM Studio is closed.
{{< /callout/note >}}

Now, go to the settings:

{{< imgs/imgc
width=""
caption="LM Studio - App settings"
alt="LM Studio - App settings"
src="@img/lm-studio-app-settings.avif" >}}

- Â«User Interface Complexity LevelÂ» - set to Â«Power UserÂ»  
- Â«Show side button labelsÂ» - enable (for clarity)  
- Â«Model loading guardrailsÂ» - your choice, I personally use Â«RelaxedÂ»  
- Â«Use LM Studio's Hugging Face ProxyÂ» - helps with accessing HF through LM Studio if a direct connection doesn't work  
- Other settings are up to you

Tap Â«DiscoverÂ» in the sidebar:

{{< imgs/imgc
width=""
caption="LM Studio - Overview of models for download"
alt="LM Studio -  Overview of models for download"
src="@img/lm-studio-discover.avif" >}}

{{< callout/hint >}}
When searching for models, don't forget to check the Â«GGUFÂ» box.
{{< /callout/hint >}}

Go to Â«RuntimeÂ»:

{{< imgs/imgc
width=""
caption="LM Studio - Overview of Runtime components"
alt="LM Studio - Overview of Runtime components"
src="@img/lm-studio-runtime-extension-packs.avif" >}}

Update Â«CPU llama.cppÂ», and the second CUDA package for Nvidia GPUs. For AMD GPUs, use Vulkan or ROCm.

On the Â«My ModelsÂ» page (in the sidebar), you can view and adjust default settings for each model.

{{< imgs/imgc
width=""
caption="LM Studio - Managing downloaded models"
alt="LM Studio - Managing downloaded models"
src="@img/lm-studio-my-models.avif" >}}

If you download models manually instead of through LM Studio, they should be placed like this:

```text
| - Text        <== Â«Models DirectoryÂ» in LM Studio
| - - dir1      <== Some dir (author)
| - - - dir2    <== Some dir (model name)
| - - - - .gguf <== Model
```

`dir1` and `dir2` don't have to be named after the author or model, the main thing is to keep this hierarchy, otherwise LM Studio won't recognize them.

Models load and unload from the top:

{{< imgs/imgc
width=""
caption="LM Studio - GUI"
alt="LM Studio - GUI"
src="@img/lm-studio-gui.avif" >}}


## Tips for KoboldCPP & SillyTavern

### KoboldCPP

{{< callout/note >}}
These tips are for KoboldCPP & llama.cpp, behavior may vary in other software.
{{< /callout/note >}}

CuBLAS usually runs faster than CLBlast for Nvidia GPUs.

For AMD GPUs, you should use Vulkan, or even better, ROCm.

---

Using Â«MMQÂ», Â«MMAPÂ», Â«MLOCKÂ», Â«ContextShiftÂ», Â«FastForwardingÂ», and Â«FlashAttentionÂ» might help speed up generation -- this is best determined through experimentation.

{{% details/1 "Explanation: MMQ, MMAP, MLOCK, ContextShift, FastForwarding, FlashAttention" %}}

**MMQ (Quantized Matrix Multiplication)**

MMQ mode uses quantized matrix multiplications for prompt processing instead of the standard cuBLAS operations. This saves VRAM and can boost performance for some quantized formats (like Q4_0), though the effect might be less noticeable for other formats.

Imagine you have two ways to do matrix calculations when generating a new token in a language model. One uses the standard cuBLAS algorithm, which already supports low precision, and the other is MMQ mode, optimized specifically for quantized data. When MMQ is enabled, the system picks the optimized algorithm for prompt processing, allowing faster request handling and reduced VRAM usage if the model is quantized at the right level. This doesn't mean cuBLAS stops being used--MMQ operates within the cuBLAS framework but changes how data is processed for certain quantized formats.

**MMAP (Memory Mapping)**

MMAP is a way of handling model weights and layers where they're loaded into RAM only partially. The system loads needed parts on demand.

When running an LLM, MMAP lets the model appear as if it's fully loaded into RAM, but in reality, only the blocks required for the current request are read.

**MLOCK (Memory Lock)**

MLOCK forces the operating system to "lock" the loaded model data in RAM, preventing it from being swapped out to disk. This avoids delays caused by accessing slower virtual memory.

For LLMs, it's important that key model weights stay in fast memory (VRAM or RAM). MLOCK pins data in RAM if part of the model is loaded there, preventing swapping and reducing latency.

**_Swapping_** is moving processes or parts of them from RAM to disk (swap file).

**ContextShift**

ContextShift allows efficient handling of large context windows. Instead of recalculating the entire context, the system "shifts" the KV cache--old data is discarded, and new data is moved to the start of the new window.

For example, if a chatbot is in a long conversation, when generating a new response, the model doesn't start processing the entire context from scratch. Instead of recomputing everything, it keeps intermediate computed representations (hidden states) of the processed messages. When generating the new reply, the model uses these saved states to quickly incorporate the latest messages. This is like incremental generation in transformers, where the model continues calculations based on what's already been processed.

This approach speeds up processing significantly, since the model focuses only on the new information.

**FastForwarding (KV-caching)**

FastForwarding lets the model skip reprocessing tokens for which hidden states (KV cache) have already been computed, generating new tokens only for changes in the sequence. This feature is especially useful when using Â«ContextShiftÂ».

Think of incremental generation in transformers: when generating a new token, the model doesn't fully recalculate all previous tokens but uses the saved KV cache for the already processed parts. FastForwarding in KoboldCPP works similarly -- if the model continues generating text in a long conversation or document, it â€œfast-forwardsâ€ through the already computed sections and updates only the latest, changed parts of the sequence, which significantly speeds up processing.

For example, if there's a history of 1000 tokens, FastForwarding creates a KV cache of those 1000 tokens in one pass, and on the next generation, it uses this cache instead of recalculating everything from scratch.

**FlashAttention**

FlashAttention uses more efficient data handling and GPU cache usage, which speeds up computations and reduces VRAM consumption.

In transformer models, attention calculations usually require a quadratic amount of operations as sequence length grows. FlashAttention optimizes this process, enabling faster processing of long sequences, which is especially important for high-load AI server applications.

{{% /details/1 %}}

---

Context also requires VRAM--and quite a bit of it--so you shouldn't load all your VRAM just with model layers if you're using a large context of 8-12K+ tokens.

Try to fit as many layers as possible into VRAM while leaving enough space for the context. If even one layer doesn't fit fully in VRAM, it's better to leave more than one layer unloaded--like 2-3 layers.

If your VRAM isn't enough to load most layers, switch to lighter quantization formats. If Q6 doesn't fit, use Q5. If Q5 doesn't fit, go for Q4.

This will reduce generation quality, but it will work. If accuracy and quality are critical, running locally might not be the best option if your hardware is insufficient.

As a last resort, if you have a lot of RAM, you can _significantly_ sacrifice speed by loading more layers into RAM than VRAM.

---

Check the generation speed with Â«BLAS Batch SizeÂ» set to 512, then set it to 2048 and check again. If 2048 is slower than 512, maybe 1024 will work better.

Ideally, the bigger the Â«BLAS Batch SizeÂ», the better, but too large a value can actually slow down generation speed.

---

Regarding Â«ThreadsÂ», the value "-1" means "auto."

If you don't manually set a value for each Â«ThreadsÂ» parameter, it will inherit the value from the main Â«ThreadsÂ» setting.

For example: you have 6 cores, 12 threads, if you set Â«ThreadsÂ» to 12 but leave Â«BLAS threadsÂ» unset, then Â«BLAS threadsÂ» will also use 12 threads. In total, that would be 24 threads used, while you only have 12 available--and the OS also needs resources.

Usually, KoboldCPP sets the optimal number of threads automatically, but it's worth checking which values are assigned by default to ensure everything is okay. You can see this next to the Â«ThreadsÂ» parameter (after the model starts) via the GUI or in the terminal logs.

If your system runs slowly or unstably, try lowering the number of threads manually.

Make sure KoboldCPP does not use more threads than you physically have, and that the system keeps some free resources (2-4+ threads) for itself.

---

To change the model, you need to restart KoboldCPP.

### SillyTavern

I couldn't find a complete guide on YouTube that explains all the SillyTavern settings in one place.

So, if you decide to stick around, you'll have to experiment and do a lot of googling... And if you're a total beginner, you'll be googling even more.

Here are your two new best friends -- [SillyTavern Documentation](https://docs.sillytavern.app/?sl) & [r/SillyTavernAI](https://www.reddit.com/r/SillyTavernAI/?sl), where you might find the info you need.

---

Models are not created in the GGUF format, GGUF is a format for storing the model's quantizations. So, wherever you download a GGUF model (often from HF), there should be a link to the "original" or instructions provided.

On the original page, you can usually find not only the model description but also settings, and in some cases, even presets for ST[^st].

[^st]: ST - short for SillyTavern.

For example, the model I'm using at the time of writing this article is Â«[NemoMix Unleashed 12B](https://huggingface.co/MarinaraSpaghetti/NemoMix-Unleashed-12B?sl)Â» ([GGUF](https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF?sl)).

Everything is described there, with settings and ST presets provided.

Actually, having the perfect settings isn't mandatory. But the right settings for the Â«Context TemplateÂ» (very important), Â«Instruct TemplateÂ» (if available), Â«System PromptÂ» (less important), and sampler settings for fine-tuning output can **VERY** significantly improve generation quality.


## Conclusion

KoboldCPP combined with SillyTavern is a powerful tool for enthusiasts who value flexibility, customization, and detailed tweaking. It's the perfect choice for experiments and RP. However, because of the many parameters and fine settings, it can take some time to get used to.

LM Studio, offers simplicity and ease of use. It's a great option for those who want to start using local models right away with minimal setup. If your goal is to generate text, code, or answers without needing fine-tuning, LM Studio is the optimal choice.

---

Read too:

{{< embedPost url="local-ai-image-generation-stability-matrix">}}
