---
draft: false
title: "Stable Diffusion"
# url: ""
description: "Stable Diffusion - Что это? Как работает? Как запустить? И другое."
summary: "Stable Diffusion - Что это? Как работает? Как запустить? И другое."
date: 2023-03-09
# lastmod: 2001-01-29
categories: ["AI"] # ["cat 1", "cat 2"]
tags: ["Статьи"] # ['tag 1', 'tag 2']
author: ["nozsh"] # ['Me', 'You'] multiple authors
# authorURL: [""] # ['link author 1', 'link author 2'], ex. ['', 'https://example.com']
# canonicalURL: "yourself"
# weight: 1
# robotsNoIndex: true

showToc: true
# TocOpen: false
# hidemeta: true
# comments: true
# disableHLJS: true
# disableShare: true
# hideSummary: true
# hideFooter: true
# searchHidden: true
# ShowCodeCopyButtons: false
# ShowReadingTime: false
# ShowWordCount: false
# ShowBreadCrumbs: false
# ShowPostNavLinks: fales
# ShowRssButtonInSectionTermList: false
# ShowCanonicalLink: true
# CanonicalLinkText: "Источник:"
# UseHugoToc: false
# hideAuthor: true
# byai: true
cover:
  image: "@img/stable-diffusion-cover.avif" # image path/url
  width: "1280" # only for img from url; EX: 1920
  height: "720" # only for img from url; EX: 1080
  alt: "Stable Diffusion - Cover" # alt text
  caption: "Cover-Image сгенерировано моделью - [Inkpunk Diffusion](https://civitai.com/models/1087/inkpunk-diffusion?sl)" # display caption under cover
  relative: true # when using page bundles set this to true
  hidden: false # only hide on current single page
---

{{< callout/note >}}
**Это старая статья и информация о установке возможно устарела!**
{{< /callout/note >}}

## О Stable Diffusion

Stable Diffusion это тип ИИ модели для генерации изображений. Так же существует одноименное ПО от [automatic1111](https://github.com/AUTOMATIC1111?sl).

Вы можете использовать любую модель какую вы только захотите на основе SD. Скачать вы их можете например тут [Civitai](/kb/qus/ai/media/civitai/).

Это статья рассказывает о Stable Diffusion, а так же объясняет как развернуть Stable Diffusion локально.

### Если вы не знаете что такое Stable Diffusion

**Stable Diffusion** - это скрытая модель диффузии текста в изображение, способная генерировать от аниме до фотореалистичных изображений при любом вводе текста, культивирующая автономную свободу для создания невероятных изображений, дающая всем возможность создавать пикчи в течение нескольких секунд.

[Выпуск Stable Diffusion](https://stability.ai/news/stable-diffusion-public-release?sl) стал важной вехой в этом развитии, поскольку высокопроизводительная модель оказалась доступной широкой публике (производительная с точки зрения качества изображения, скорости и относительно низких требований к ресурсам).

{{< details/1html "Пикчи" >}}
{{< imgs/gallerya
"@img/pic-001-stable-diffusion-djz-ink-punk-v21-grid.avif" ""
"@img/pic-002-stable-diffusion-inkpunk-diffusion-v2-grid.avif" ""
"@img/pic-003-stable-diffusion-djz-dieselpunk-city-v21-grid.avif" ""
"@img/pic-004-stable-diffusion-djz-dieselpunk-city-v21.avif" "big"
"@img/pic-005-stable-diffusion-umiaimythologyandbabes.avif" "big"
"@img/pic-006-stable-diffusion-umiaimythologyandbabes.avif" ""
"@img/pic-008-stable-diffusion-umiaimythologyandbabes.avif" ""
"@img/pic-009-stable-diffusion-umiaimythologyandbabes.avif" ""
"@img/pic-010-stable-diffusion-umiaimythologyandbabes.avif" ""
"@img/pic-011-stable-diffusion-umiaimythologyandbabes.avif" "" >}}

<p><b>Модели:</b></p>

<ul>
<li>
<a href="https://civitai.com/models/4003/djz-ink-punk-v21?sl">djz Ink Punk V21</a>
</li>
<li>
<a href="https://civitai.com/models/3985/djz-dieselpunk-city-v21?sl">djz Dieselpunk City V21</a>
</li>
<li>
<a href="https://civitai.com/models/1087/inkpunk-diffusion?sl">Inkpunk Diffusion</a>
</li>
<li>
<a href="https://civitai.com/models/6077/umi-ai-mythology-and-babes-by-dutchalex?sl">Umi AI Mythology and Babes by DutchAlex</a>
</li>
</ul>
{{< /details/1html >}}

## Техническая часть

В этой части я покажу вам как же все таки запустить и опробовать эту шайтан машину. А так же тут будет очень много текста, который поясняет как это работает. Но читать его не обязательно.

### Как работает Stable Diffusion

![Stable Diffusion - Как работает Stable Diffusion (1)](001-stable-diffusion-kak-rabotayet-stable-diffusion.avif)

Stable Diffusion гибка, то есть может использоваться множеством разных способов. Давайте сначала рассмотрим генерацию изображений на основе одного текста (text2img). На картинке выше показан пример текстового ввода и получившееся сгенерированное изображение.

Кроме превращения текста в изображение, другим основным способом применения модели является изменение изображений (то есть входными данными становятся текст + изображение).

![Stable Diffusion - Как работает Stable Diffusion (2)](002-stable-diffusion-kak-rabotayet-stable-diffusion.avif)

Давайте начнем разбираться со внутренностями модели, потому что это поможет нам объяснить ее компоненты, их взаимодействие и значение опций/параметров генерации изображений.

#### Компоненты Stable Diffusion

Stable Diffusion - это система, состоящая из множества компонентов и моделей. Это не единая монолитная модель.

Изучая внутренности, мы первым делом заметим, что в модели есть компонент понимания текста, преобразующий текстовую информацию в цифровой вид, который передает заложенный в текст смысл.

![Stable Diffusion - Компоненты Stable Diffusion (1)](003-stable-diffusion-komponenty-stable-diffusion.avif)

Мы начнем с общего обзора, а позже углубимся в подробности машинного обучения. Однако для начала можно сказать, что этот кодировщик текста - это специальная языковая модель Transformer (технически ее можно описать как текстовый кодировщик модели CLIP). Она получает на входе текст и выдает на выходе список чисел (вектор), описывающий каждое слово/токен в тексте.

Далее эта информация передается генератору изображений, который состоит из двух компонентов.

![Stable Diffusion - Компоненты Stable Diffusion (2)](004-stable-diffusion-komponenty-stable-diffusion.avif)

Генератор изображений выполняет два этапа:

**1. Создание информации изображения**

Этот компонент является секретным ингредиентом Stable Diffusion. Именно благодаря нему возник такой рост качества по сравнению с предыдущими моделями.

Этот компонент выполняется в несколько шагов (step), генерируя информацию изображения. Это параметр steps в интерфейсах и библиотеках Stable Diffusion, который часто по умолчанию имеет значение 50 или 100.

Этап создания информации изображения действует полностью в пространстве информации изображения (или в скрытом пространстве). Подробнее о том, что это значит, будет ниже. Это свойство ускоряет работу по сравнению с предыдущими моделями диффузии, работавшими в пространстве пикселей. Этот компонент состоит из нейросети UNet и алгоритма планирования.

Слово «диффузия» (diffusion) описывает происходящее в этом компоненте. Это пошаговая обработка информации, приводящая в конечном итоге к генерации высококачественного изображения (при помощи следующего компонента – декодера изображений).

![Stable Diffusion -  Создание информации изображения](005-stable-diffusion-sozdaniye-informatsii-izobrazheniya.avif)

**2. Декодер изображений**

Декодер изображений рисует картину на основе информации, которую он получил на этапе создания информации. Он выполняется только один раз в конце процесса и создает готовое пиксельное изображение.

![Stable Diffusion -  Декодер изображений](006-stable-diffusion-diffusion-dekoder-izobrazheniy.avif)

На изображении выше мы видим три основных компонента (каждый со своей собственной нейросетью), из которых состоит Stable Diffusion:

- **ClipText** для кодирования текста

  - Входные данные: текст
  - Выходные данные: 77 векторов эмбеддингов токенов, каждый в 768 измерениях

- **UNet + Scheduler** для постепенной обработки/диффузии информации в пространстве информации (скрытом пространстве)

  - Входные данные: эмбеддинги текста и исходный многомерный массив (структурированные списки чисел, также называемые _тензором_), состоящий из шума
  - Выходные данные: массив обработанной информации

- **Декодер автокодировщика (Autoencoder decoder)**, рисующий готовое изображение при помощи массива обработанной информации
  - Входные данные: массив обработанной информации (размеры: (4,64,64))
  - Выходные данные: готовое изображение (размеры: (3, 512, 512) – (красный/зеленый/синий, ширина, высота))

![Stable Diffusion - Декодер изображений](007-stable-diffusion-diffusion-dekoder-izobrazheniy.avif)

##### Что такое диффузия?

Диффузия - это процесс, выполняемый внутри розового компонента «image information creator» (этапа создания информации изображения). Имея эмбеддинги токенов, описывающие введенный текст, и случайный начальный _массив информации изображения_ (также они называются _latent_), процесс создает массив информации, который декодер изображения использует для рисования готового изображения.

![Stable Diffusion - Что такое диффузия](008-stable-diffusion-diffusion-chto-takoye-diffuziya.avif)

Это процесс выполняется поэтапно. Каждый шаг добавляет больше релевантной информации. Чтобы представить процесс в целом, мы можем изучить массив случайных latent, и увидеть, что он преобразуется в визуальный шум. В данном случае визуальное изучение – это прохождение данных через декодер изображений.

![Stable Diffusion - Что такое диффузия](009-stable-diffusion-diffusion-chto-takoye-diffuziya.avif)

Диффузия выполняется в несколько шагов, каждый из которых работает с входным массивом latent и создает еще один массив latent, еще больше напоминающий введенный текст, а вся визуальная информация модели собирается из всех изображений, на которых была обучена модель.

![Stable Diffusion - Что такое диффузия](010-stable-diffusion-diffusion-chto-takoye-diffuziya.avif)

Мы можем визуализировать набор таких latent, чтобы увидеть, какая информация добавляется на каждом из шагов.

![Stable Diffusion - Что такое диффузия](011-stable-diffusion-diffusion-chto-takoye-diffuziya.avif)

{{< vids/vid
width=""
align="center"
attr="controls mute loop"
controlslist=""
webm="@vid/diffusion-steps-all-loop.webm"
mp4=""
preload="metadata"
poster="" >}}

{{< details/1 "YouTube: diffusion steps all loop" >}}
{{< iframes/yt AhCq7YkC7Uo "diffusion steps all loop" >}}
{{< /details/1 >}}

В данном случае нечто особо восхитительное происходит между шагами 2 и 4. Как будто контур возникает из шума.

{{< vids/vid
width=""
align="center"
attr="controls mute loop"
controlslist=""
webm="@vid/stable-diffusion-steps-2-4.webm"
mp4=""
preload="metadata"
poster="" >}}

{{< details/1 "YouTube: stable diffusion steps 2 4" >}}
{{< iframes/yt obYWByf40OQ "" >}}
{{< /details/1 >}}

##### Как работает диффузия

Основная идея генерации изображений при помощи диффузионной модели использует тот факт, что у нас есть мощные модели компьютерного зрения. Если им передать достаточно большой массив данных, эти модели могут обучаться сложным операциям. Диффузионные модели подходят к задаче генерации изображений, формулируя задачу следующим образом:

Допустим, у нас есть изображение, сделаем первый шаг, добавив в него немного шума.

![Stable Diffusion - Как работает диффузия](012-stable-diffusion-diffusion-kak-rabotayet-diffuziya.avif)

Назовем «срез» (slice) добавленного нами шума «noise slice 1». Сделаем еще один шаг, добавив к шумному изображению еще шума («noise slice 2»).

![Stable Diffusion - Как работает диффузия](013-stable-diffusion-diffusion-kak-rabotayet-diffuziya.avif)

На этом этапе изображение полностью состоит из шума. Теперь давайте возьмем их в качестве примеров для обучения нейронной сети компьютерного зрения. Имея номер шага и изображение, мы хотим, чтобы она спрогнозировала, сколько шума было добавлено на предыдущем шаге.

![Stable Diffusion - Как работает диффузия](014-stable-diffusion-diffusion-kak-rabotayet-diffuziya.avif)

Хотя этот пример показывает два шага от изображения к полному шуму, мы можем управлять тем, сколько шума добавляется к изображению, поэтому можно распределить его на десятки шагов, создав десятки примеров для обучения на каждое изображение для всех изображений в обучающем массиве данных.

![Stable Diffusion - Как работает диффузия](015-stable-diffusion-diffusion-kak-rabotayet-diffuziya.avif)

Красота здесь в том, что после того, как эта сеть прогнозирования шума начнет работать правильно, она, по сути, сможет рисовать картины, удаляя шум на протяжении множества шагов.

Примечание: это небольшое упрощение алгоритма диффузии. На ресурсах по ссылкам в конце статьи представлено более подробное математическое описание.

##### Рисование изображений устранением шума

Обученный предсказатель шума может взять шумное изображение и количество шагов устранения шума, и на основании этого способен спрогнозировать срез шума.

![Stable Diffusion - Рисование изображений устранением шума](016-stable-diffusion-diffusion-risovaniye-izobrazheniy-ustraneniyem-shuma.avif)

Срез шума прогнозируется таким образом, что если мы вычтем его из изображения, то получим изображение, которое ближе к изображениям, на которых обучалась модель.

![Stable Diffusion - Рисование изображений устранением шума](017-stable-diffusion-diffusion-risovaniye-izobrazheniy-ustraneniyem-shuma.avif)

Если обучающий массив данных состоял из эстетически приятных изображений (например, [LAION Aesthetics](https://laion.ai/blog/laion-aesthetics/?sl), на котором обучалась Stable Diffusion), то получившееся изображение будет иметь склонность к эстетической приятности.

![Stable Diffusion - Рисование изображений устранением шума](018-stable-diffusion-diffusion-risovaniye-izobrazheniy-ustraneniyem-shuma.avif)

В этом по большей мере и заключается описание генерации изображений диффузионными моделями, представленное в статье [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239?sl). Теперь, когда мы понимаем, что такое диффузия, нам понятно, как работают основные компоненты не только Stable Diffusion, но и Dall-E 2 с Google Imagen.

Обратите внимание, что описанный выше процесс диффузии генерирует изображения без использования текстовых данных. В последующих разделах будет рассказано, как в процесс внедряется текст.

#### Увеличение скорости: диффузия сжатых (скрытых) данных, а не пиксельного изображения

Для ускорения процесса генерации изображений Stable Diffusion (по информации из исследовательской статьи) выполняет процесс диффузии не с самими пиксельными изображениями, а со сжатой версией изображения. В [статье](https://arxiv.org/abs/2112.10752?sl) это называется «переходом в скрытое пространство».

Это сжатие (и последующая распаковка/рисование) выполняется при помощи автокодировщика. Автокодировщик сжимает изображение в скрытое пространство при помощи своего кодировщика, а затем воссоздает его при помощи декодера на основе только сжатой информации.

![Stable Diffusion - Увеличение скорости: диффузия сжатых (скрытых) данных, а не пиксельного изображения](<019-stable-diffusion-uvelicheniye-skorosti-diffuziya-szhatykh-(skrytykh)-dannykh-a-ne-pikselnogo-izobrazheniya.avif>)

Далее со сжатыми latent выполняется прямой процесс диффузии. Используются срезы шума, применяемые к этим latent, а не к пиксельному изображению. То есть предсказатель шума на самом деле обучается прогнозировать шум в сжатом описании (в скрытом пространстве).

![Stable Diffusion - Увеличение скорости: диффузия сжатых (скрытых) данных, а не пиксельного изображения](<020-stable-diffusion-uvelicheniye-skorosti-diffuziya-szhatykh-(skrytykh)-dannykh-a-ne-pikselnogo-izobrazheniya.avif>)

При помощи прямого процесса (с использованием кодировщика автокодировщика) мы генерируем данные для обучения предсказателя шума. После его обучения мы можем генерировать изображения, выполняя обратный процесс (при помощи декодера автокодировщика).

![Stable Diffusion - Увеличение скорости: диффузия сжатых (скрытых) данных, а не пиксельного изображения](<021-stable-diffusion-uvelicheniye-skorosti-diffuziya-szhatykh-(skrytykh)-dannykh-a-ne-pikselnogo-izobrazheniya.avif>)

Эти два потока показаны на рисунке статьи про LDM/Stable Diffusion:

![Stable Diffusion - Увеличение скорости: диффузия сжатых (скрытых) данных, а не пиксельного изображения](<022-stable-diffusion-uvelicheniye-skorosti-diffuziya-szhatykh-(skrytykh)-dannykh-a-ne-pikselnogo-izobrazheniya.avif>)

Также на этом рисунке показаны компоненты «согласования», которые в данном случае являются текстовыми строками, описывающими изображение, которое должна генерировать модель. Поэтому давайте рассмотрим эти текстовые компоненты.

##### Текстовый кодировщик: языковая модель Transformer

Языковая модель Transformer используется в качестве компонента понимания языка, она получает текстовую строку и создает эмбеддинги токенов. В опубликованной модели Stable Diffusion используется ClipText ([модель на основе GPT](https://jalammar.github.io/illustrated-gpt2/?sl)), а в статье применяется [BERT](https://jalammar.github.io/illustrated-bert/?sl).

В статье, посвященной Imagen, показано, что выбор языковой модели важен. Замена на более объемные языковые модели сильнее влияет на качество генерируемого изображения, чем более объемные компоненты генерации изображений.

{{< imgs/imgc width="" caption="Улучшение/увеличение языковых моделей существенно влияет на качество моделей генерации изображений.<br>Источник: <a href='https://arxiv.org/abs/2205.11487?sl'>Статья про Google Imagen, написанная Saharia и соавторами.</a>" alt="Текстовый кодировщик: языковая модель Transformer (1)" src="023-stable-diffusion-tekstovyy-kodirovshchik-yazykovaya-model-transformer.avif" >}}

В первых моделях Stable Diffusion просто подключалась предварительно обученная модель ClipText, выпущенная OpenAI. Возможно, будущие модели перейдут на новые и гораздо более объемные [OpenCLIP](https://laion.ai/blog/large-openclip/?sl)-варианты CLIP. В эту новую группу входных векторов включены текстовые модели размерами до 354 миллионов параметров, в отличие от 63 миллионов параметров в ClipText.

###### Как обучается CLIP

CLIP обучается на массиве изображений и подписей к ним. Массив данных выглядит примерно так, только состоит из 400 миллионов изображений и подписей:

![Stable Diffusion - Как обучается CLIP](024-stable-diffusion-kak-obuchayetsya-CLIP.avif)

CLIP - это сочетание кодировщика изображений и кодировщика текста. Обучающий процесс модели можно упрощенно представить как кодирование изображения и его подписи кодировщиками изображений и текста.

![Stable Diffusion - Как обучается CLIP](025-stable-diffusion-kak-obuchayetsya-CLIP.avif)

Затем мы сравниваем получившиеся эмбеддинги при помощи косинусного коэффициента. В начале процесса обучения схожесть будет низкой, даже если тест описывает изображение правильно.

![Stable Diffusion - Как обучается CLIP](026-stable-diffusion-kak-obuchayetsya-CLIP.avif)

Мы обновляем две модели так, чтобы в следующий раз при создании эмбеддингов получившиеся эмбеддинги были схожими.

![Stable Diffusion - Как обучается CLIP](027-stable-diffusion-kak-obuchayetsya-CLIP.avif)

Повторяя этот процесс со всем массивом данных и группами входных векторов большого размера, мы получаем кодировщики, способные создавать эмбеддинги, в которых изображение собаки и предложение «a picture of a dog» схожи. Как и в [word2vec](https://jalammar.github.io/illustrated-word2vec/?sl), процесс обучения также должен включать в себя **отрицательные примеры** изображений и подписей, которые не совпадают, а модель должна присваивать им низкую оценку схожести.

#### Передача текстовой информации в процесс генерации изображений

Чтобы сделать текст частью процесса генерации изображений, нам нужно модифицировать предсказатель шума так, чтобы он использовал в качестве входных данных текст.

![Stable Diffusion - Передача текстовой информации в процесс генерации изображений](028-stable-diffusion-peredacha-tekstovoy-informatsii-v-protsess-generatsii-izobrazheniy.avif)

Теперь наш массив данных содержит закодированный текст. Так как мы работаем в скрытом пространстве, то входные изображения и прогнозируемый шум находятся в скрытом пространстве.

![Stable Diffusion - Передача текстовой информации в процесс генерации изображений](029-stable-diffusion-peredacha-tekstovoy-informatsii-v-protsess-generatsii-izobrazheniy.avif)

Чтобы лучше понять, как текстовые токены используются в Unet, давайте глубже разберемся с Unet.

##### Слои предсказателя шума Unet (без текста)

Для начала рассмотрим диффузионную Unet, не использующую текст. Ее входы и выходы выглядят так:

![Stable Diffusion - Слои предсказателя шума Unet (без текста)](030-stable-diffusion-sloi-predskazatelya-shuma-unet-bez-teksta.avif)

Внутри мы видим следующее:

- Unet - это последовательность слоев, работающая над преобразованием массива latent.
- Каждый слой обрабатывает выходные данные предыдущего слоя.
- Часто выходных данных подается (через остаточные соединения) для обработки на дальнейших этапах сети.
- Шаг времени преобразуется в вектор эмбеддингов шага времени, который используется в слоях.

![Stable Diffusion - Слои предсказателя шума Unet (без текста)](031-stable-diffusion-sloi-predskazatelya-shuma-unet-bez-teksta.avif)

##### Слои предсказателя шума Unet с текстом

Давайте посмотрим, как изменить эту систему, чтобы уделить внимание тексту.

![Stable Diffusion - Слои предсказателя шума Unet с текстом](032-stable-diffusion-sloi-predskazatelya-shuma-unet-s-tekstom.avif)

Основное изменение системы, которое необходимо для добавления поддержки текстового ввода (техническое название: text conditioning) – добавление слоя attention между блоками ResNet.

![Stable Diffusion - Слои предсказателя шума Unet с текстом](033-stable-diffusion-sloi-predskazatelya-shuma-unet-s-tekstom.avif)

Обратите внимание, что блок resnet не смотрит непосредственно на текст. Слои attention объединяют эти текстовые описания в latent. И теперь следующий ResNet может использовать эту встроенную текстовую информацию в своей обработке.

#### Заключение

Надеюсь, это даст вам поверхностное понимание работы Stable Diffusion. В ней задействовано множество других концепций, но я считаю, что их проще понять, если вы знаете описанные выше строительные блоки. Для дальнейшего изучения можно воспользоваться представленными ниже полезными ресурсами.

#### Ресурсы

- [Stable Diffusion with 🧨 Diffusers](https://huggingface.co/blog/stable_diffusion?sl)
- [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion?sl)
- [How does Stable Diffusion work? – Latent Diffusion Models EXPLAINED](https://www.youtube.com/watch?v=J87hffSMB60?sl) (Видео)
- [Stable Diffusion - What, Why, How?](https://www.youtube.com/watch?v=ltLNYA3lWAQ?sl) (Видео)
- [High-Resolution Image Synthesis with Latent Diffusion Models](https://ommer-lab.com/research/latent-diffusion-models/?sl) (Статья про Stable Diffusion)
- Более подробное изучение алгоритмов и математики представлено в статье Лилиан Венг (Lilian Weng) [What are Diffusion Models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/?sl)
- [Одноминутный клип на YouTube](https://youtube.com/shorts/qL6mKRyjK-0?sl) по использованию [Dream Studio](https://beta.dreamstudio.ai/?sl) для генерации изображений при помощи Stable Diffusion.
- Хорошие [видео](https://www.youtube.com/playlist?list=PLfYUBJiXbdtRUvTUYpLdfHHp9a58nWVXP?sl) о Stable Diffusion от fast.ai

Источник - перевод «[The Illustrated Stable Diffusion](https://jalammar.github.io/illustrated-stable-diffusion/?sl)»

### Запуск Stable Diffusion (Возможно устарело)

Что нужно для запуска локально? Мощное железо.

В самом деле если вы собрали/купили свой ПК не 20 лет назад, вероятней всего вашего железа хватит.

Кто то пишет что нужно минимум 6Gb VRAM, кто то 10Gb VRAM. А кто то что нужна строго карта от зеленых. Но вот вам правда на момент написания статьи и Stable Diffusion 2.1: нужно минимум 6VRAM, 8RAM и не обязательно иметь карту от Nvidia, а так же серии RTX. Stable Diffusion можно запустить на любой карте будь то AMD, Nvidia или даже Intel.

Насчет процессора как таковой информации нету. Но при желании Stable Diffusion можно запустить и на 4VRAM. Используя облегченные форки например от [basujindal](https://github.com/basujindal/stable-diffusion?sl).

Так же стоит предупредить что [официальная установка Stable Diffusion](https://github.com/CompVis/stable-diffusion?sl) и правда требует минимум 10VRAM и 16RAM.

Как нынче все что можно запустить в этом мире, моя рекомендация запускать конечно же на SSD.

Мы будем использовать вариант от [AUTOMATIC1111](https://github.com/AUTOMATIC1111?sl), простой, удобный и не слишком требовательный.

То есть понадобится примерно 6VRAM и 16RAM. На выходе мы получим удобный веб интерфейс на локальном хосте, и даже плюсом сможем расшарить его и поделится ссылкой с кем то.

#### Установка Stable Diffusion локально

Для начало вам нужно будет скачать или обновить или сделать downgrade Python, на текущий момент строго версии [3.10.x](https://www.python.org/downloads/release/python-3109/?sl), тк Pytorch не поддерживает версию 3.11.x. Посмотреть нужную версию питона можно [в репозитории на GitHub](https://github.com/AUTOMATIC1111/stable-diffusion-webui#user-content-automatic-installation-on-windows?sl). На патч версию можно не обращать внимания (в большинстве случаев).

{{< callout/warn >}}
**ВАЖНО:** Запустите установщик **от имени админа**, установите **ДЛЯ ВСЕХ пользователей** и не забудьте поставить галочку на **Add path.**
{{< /callout/warn >}}

Качаем или клонируем репозиторий: [stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui) (если вы на Linux, лучше клонировать).

Далее переходим на [HuggingFace](https://huggingface.co/stabilityai/stable-diffusion-2-1?sl). Там вам нужно будет скачать саму модель в формате ckpt или safetensors, не имеет значения в каком.

{{< details/1 "В чем отличие: ckpt, safetensors; ema, noema; 512, 768;" >}}
**ckpt** от **safetensors**

Они одинаковы, но **ckpt** может содержать вредоносный код, **safetensors** нет.

---

**ema** от **nonema**

**EMA** - можно обучать дальше. **NONEMA** - нельзя.

---

**512** от **768**

Это размеры картинок, на которых обучалась модель. На практике 768 работает лучше.

Сравнение можно посмотреть [здесь](https://huggingface.co/stabilityai/stable-diffusion-2/discussions/22?sl).
{{< /details/1 >}}

Далее скачайте [v2-inference-v.yaml](https://github.com/Stability-AI/stablediffusion/tree/main/configs/stable-diffusion?sl).

{{< iframes/yt e3vcYVwEkW0 "NEW Stable Diffusion 2.1 Tutorial - easy setup + what you need to know" "?start=124" >}}

И так что мы имеем?

- Вы установили Python, и если вы введете в консоль `python -V` или `python3 -V`, вы должны увидеть версию питона, указанную выше или в [репозитории на GitHub](https://github.com/AUTOMATIC1111/stable-diffusion-webui#user-content-automatic-installation-on-windows?sl).
- Вы скачали репозиторий с гитхаба.
- Вы скачали модель Stable Diffusion, а так же конфиг файл.

Если все так, идем дальше.

Открываем `../stable-diffusion-webui/models/Stable-diffusion/` – и забрасываем туда модель и конфиг файл.

Далее копируем название модели, и переименовываем конфиг файл в название модели.

_Например: v2-1_768-nonema-pruned.safetensors >> v2-1_768-nonema-pruned.yaml._

Далее в корневой папки откройте с помощью текстового редактора файл - **webui-user.bat**, и измените аргументы запуска (`set COMMANDLINE_ARGS=`), но сначала изучите информацию ниже!

```bash
# Базовые аргументы, нужные для работы
--api --cors-allow-origins=https://www.painthua.com --no-half

# Можете добавить это, что бы расшарить localhost
--share --listen

# Так же можно добавить, об этом ниже
--xformers

# Максимальная оптимизация, позволит вам использовать более высокое разрешение, но повлияет на скорость генерации
--xformers --opt-split-attention --opt-sub-quad-attention --medvram --api --cors-allow-origins=https://www.painthua.com --no-half
```

Иногда генерируется сомнительная пикча, потому что разрешение слишком маленькое. Поэтому следует использовать последнюю строку параметров запуска.

Для запуска нужно использовать **webui-user.bat**.

#### Аргументы запуска

##### Xformers

Библиотека Xformers - это дополнительный способ ускорить создание изображений. Но это фича доступна только для видеокарт Nvidia.

Для использования понадобится указать `--xformers` в параметрах запуска. А так же скачать и установить [CUDA 11.3](https://developer.nvidia.com/cuda-11.3.0-download-archive?sl) (какую версию CUDA смотреть [здесь](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers#user-content-building-xformers-on-windows-by-duckness?sl)), выбрать Custom, а далее выбрать только следующее:

![Установка Stable Diffusion локально - CUDA](034-stable-diffusion-xformers-installer.avif)

`--force-enable-xformers` - Включает xformers, независимо от того, может ли он запустится или нет. Не сообщает об ошибках, которые могут появится, во время выполнения.

Подробнее [здесь](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers?sl).

##### SHARE и LISTEN

`--share` - создает публичную ссылку на gradio.

`--listen` - должен расшарить локалхост внутри локальной сети, но лично у меня не работает.

Из-за параметров **share** и **listen** могу не работать некоторые расширения, которые скорее всего еще не установлены, но которые вы возможно захотите установить, например [ControlNet](https://huggingface.co/lllyasviel/ControlNet?sl) (у меня так).

##### Оптимизация

| Аргумент                        | Пояснение                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| :------------------------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `--opt-split-attention`         | Использует черную магию, для снижения памяти почти без затрат.                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| `--disable-opt-split-attention` | Отключает вышеуказанную оптимизацию.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| `--opt-sub-quad-attention`      | Субквадратичное внимание - оптимизация слоя Cross Attention, эффективная для памяти, которая может значительно уменьшить требуемую память, иногда с небольшими затратами производительности. Рекомендуется при низкой производительности или неудачных генерациях с аппаратной/программной конфигурацией, для которой xformers не работает.                                                                                                                                                                                         |
| `--opt-split-attention-v1`      | Использует более старую версию оптимизации, которая не так требовательна к памяти (она будет использовать меньше VRAM, но будет более ограничена в максимальном размере изображений, которые вы можете сделать).                                                                                                                                                                                                                                                                                                                    |
| `--medvram`                     | Заставляет модель Stable Diffusion потреблять меньше VRAM, разделяя ее на три части – cond (для преобразования текста в числовое представление), first_stage (для преобразования изображения в латентное пространство и обратно) и unet (для собственно деноизации латентного пространства) и делая так, чтобы только одна из них постоянно находилась в VRAM, отправляя остальные в CPU RAM. Это снижает производительность, но незначительно – за исключением случаев, когда включен предварительный просмотр в реальном времени. |
| `--lowvram`                     | Еще более тщательная оптимизация вышеописанного, разбиение unet на множество модулей, и только один модуль хранится в VRAM. Разрушительно для производительности.                                                                                                                                                                                                                                                                                                                                                                   |
| **\*do-not-batch-cond-uncond**  | Предотвращает пакетную обработку положительных и отрицательных подсказок во время выборки, что позволяет работать с размером партии 0,5, экономя много памяти. Снижает производительность. Не опция командной строки, а оптимизация, неявно включаемая при использовании `--medvram` или `--lowvram`.                                                                                                                                                                                                                               |
| `--always-batch-cond-uncond`    | Отключает вышеуказанную оптимизацию. Имеет смысл только вместе с `--medvram` или `--lowvram`                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| `--opt-channelslast`            | Изменяет тип памяти torch для стабильной диффузии в каналы в последнюю очередь. Эффекты не изучены.                                                                                                                                                                                                                                                                                                                                                                                                                                 |

Подробнее [здесь](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Optimizations?sl).

#### Промты (Подсказки)

**Positive** - верхнее поле, туда вы вводите чтобы вы хотели.

**Negative** - нижние поле, туда вы вводите чтобы вы **НЕ** хотели.

**()** - Так же вы можете использовать "приоритет" – вес\акцент промта, который будет применятся, "сильнее".

- (1)
- ((2))
- (((3)))
- ((((4))))
- (((((5)))))
- ((((((6))))))

**[]** - Меньшей "приоритет" – уменьшение веса\акцента промта, который будет применятся, "слабее".

- [1]
- [[2]]
- [[[3]]]
- [[[[4]]]]
- [[[[[5]]]]]
- [[[[[[6]]]]]]

**Соотношения** - …:1.0, указывает силу\акцент\вес в дробях.

Например: (Blue hair) будет иметь больший вес, чем [Blue hair] в конечном результате, (Blue hair:1.4) увеличит Blue hair на ~40% больше, чем обычно, (Blue hair:0.6) уменьшит их на ~40%.

#### Models, Lora, Textual Inversion, Hypernetwork

##### Checkpoint (Модель)

**Модель** она же **«checkpoint»**. Большая, очень большая или не очень большая уже обученная модель.

Генерируют txt2img, img2img, etc. с нуля, исходя из больших данных, которыми обучали модель.

Предоставляется в ckpt и/или safetensors.

Для установки нужно закинуть ее в `../stable-diffusion-webui/models/Stable-diffusion/`.

##### LoRA

**LoRA (Low-Rank Adaptation)** - это грубо говоря мини-модель. Суть лоры в том что бы добавить "фичу" на пикчу сгенерированную полноценной моделью.

Основная идея LoRA заключается в том, чтобы адаптировать уже обученные модели к новым задачам или данным, не переобучая всю модель, а добавляя лишь небольшое количество параметров.

Аналогично моделям это ckpt и/или safetensors, но весит очень мало в сравнение с моделью.

Для установки нужно закинуть в `../stable-diffusion-webui/models/Lora/`.

Для использования в webui нужно нажать на красную кнопку (🎴), выбрать Lora, обновить список, далее выбрать нужную.

##### Textual Inversion

**Textual Inversion** ака «**embedding**» – это почти как Lora. Грубо говоря что-то типа стиля.

{{< details/1 "Подробнее" >}}
**Textual Inversion** - это метод, который позволяет создавать специальные эмбеддинги для определенных концепций или объектов, используя текстовые описания. Этот подход часто применяется в области генеративных моделей, таких как Stable Diffusion, чтобы улучшить качество генерации изображений или текста.

Суть метода заключается в том, что вместо того чтобы использовать стандартные текстовые эмбеддинги, мы обучаем модель на конкретных примерах, чтобы она могла ассоциировать определенные слова или фразы с уникальными представлениями (эмбеддингами). Например, если мы хотим, чтобы модель знала, как визуализировать конкретного персонажа или стиль, мы можем предоставить ей набор примеров, и она создаст эмбеддинг, который будет представлять этот концепт.

Эмбеддинги, созданные с помощью Textual Inversion, позволяют более точно настраивать результаты генерации, придавая им желаемые характеристики. Это полезно для персонализации контента и улучшения качества генерации в специфических областях.
{{< /details/1 >}}

Обычно это pt или bin (первый формат используется автором оригинала, второй – библиотекой диффузоров) с вложением в него.

Для установки нужно закинуть в `../stable-diffusion-webui/embeddings/`.

Использование в webui аналогично Lora.

##### Hypernetwork

**Hypernetwork** - это, грубо говоря, что-то похожее на Textual Inversion. С помощью Textual Inversion вы направляете нейросеть к конкретным целям, тогда как Hypernetwork учитывает прошлые результаты генерации для будущих выходов.

Что известно, так это то, что Hypernetwork может восприниматься как некий "оригинальный" промт для создания изображений. Но на практике для конечного пользователя Lora, Textual Inversion и Hypernetwork часто могут казаться аналогичными инструментами.

Для установки нужно закинуть в .`./stable-diffusion-webui/models/hypernetworks/`.

Использование в webui аналогично Lora и Textual Inversion.

#### Сэмплирование

**Sampling method** - это метод обработки входного шума, и разные методы обрабатывают его по-разному.

Различные значения **Sampling steps** значительно влияют на внешний вид результата. Обычно увеличение количества шагов добавляет детали и делает изображение более качественным. Однако это не всегда работает: иногда картинке не нужно столько деталей, особенно если модель рассчитана на арт или абстракцию. В некоторых случаях, из-за слишком большого количества деталей, изображение может стать гиперреалистичным, несмотря на изначальную абстракцию.

На практике **Sampling steps** особенно сильно влияют на генерацию существ (людей, животных и т.д.) и предметов.

Если вам нужен быстрый ответ и не хочется разбираться: **Euler** и **Euler a** - это дефолтные методы, которые в 80% случаев дают результат от "так себе" до "сверх круто". **DDIM**, **DPM++ 2M Karras** и **DPM++ SDE Karras** - это самые качественные методы, особенно для генерации персонажей и просторных пейзажей.

## Как пользоваться Stable Diffusion WebUI

### Интерфейс

#### Sampling method и Sampling steps

Про Sampling method и Sampling steps смотреть <a href="#сэмплирование">выше</a>.

#### Restore faces

Опция восстанолвения лиц. Обычно плохо работает на артах, абстракциях и подобных (даже если там есть лица).

#### Tiling

**Tiling** - это метод, который делает изображение таким, чтобы его грани не содержали резких или четких элементов, что позволяет размещать несколько таких изображений рядом друг с другом без видимых швов.

В общем смысле это можно назвать текстурой, хотя это довольно упрощенное определение. Tiling применяется не только для создания текстур или фонов. Например, вы могли видеть персонажей, вписанных в фон не с помощью света, а через использование частиц, что также может быть достигнуто с помощью этого метода.

#### Hires. fix

Это очень полезная опция, но, как правило, она не так критична для пейзажей. Модель обычно обучается на изображениях размером 512px или 768px.

Если вы сгенерируете лицо человека в разрешении 512px, то результат будет ожидаемым. Однако при генерации в 1K+ вы можете получить неожиданный результат. Скорее всего, сгенерируется "мутант" с искажённым лицом, или, в лучшем случае, может возникнуть несколько лиц, которые растянутся в ширину или длину в зависимости от размера изображения.

Поэтому, если вы хотите получить высококачественное изображение лица без искажений, используйте **Hires. fix** с минимальным значением апскейла, равным 1 (то есть без увеличения). Рекомендуемый шаг - 0.5. И помните, что эта опция использует VRAM.

#### Batch count и Batch size

**Batch count** - это количество изображений, которые будут сгенерированы за одно нажатие. Он не влияет на потребление VRAM, так как изображения генерируются по очереди, но влияет на общее время завершения генерации. Вы можете открыть папку output и увидеть уже сгенерированные изображения из этой очереди.

**Batch size** - это то же самое, что и Batch count, но в отличие от него, изображения генерируются одновременно. Это влияет на потребление VRAM и требует больше ресурсов.

Например, если Batch count равен 3, то сначала будет сгенерировано одно изображение, затем следующее и так далее. В случае Batch size равного 3 будет сгенерировано сразу три изображения.

#### CFG Scale

Это параметр, который определяет, насколько строго генерация будет следовать вашим промтам. Чем выше значение, тем более точно результат будет соответствовать заданным описаниям, но при этом может снижаться уникальность изображения.

Грубый пример:

Если вы задаете низкое значение CFG Scale и просите сгенерировать дерево и гору, может получиться изображение с лесом и без горы, только гора или картинка с множеством деталей, где присутствуют дерево и гора, но не в центре внимания.

На высоком значении CFG Scale результатом будет более точное соответствие запросу: сгенерируется именно дерево и гора.

Примерные значения:

- 0 - 6: Вы позволяете модели игнорировать ваши промты, что может привести к неожиданным результатам.
- 6 - 10: Вы хотите, чтобы модель следовала вашим промтам, но допускаете небольшие вариации в результате.
- 10+: Вы требуете, чтобы модель строго придерживалась ваших промтов, без отклонений.
